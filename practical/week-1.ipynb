{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ba74b6-0f42-458e-b799-404620a841dd",
   "metadata": {},
   "source": [
    "# Introduction to data analysis in Python: Week 1\n",
    "\n",
    "We're going to be working with the Squirrel Census dataset this week. [The Squirrel Census](https://www.thesquirrelcensus.com/) is a multimedia science, design, and storytelling project focusing on the Eastern gray (Sciurus carolinensis). The dataset contains data for 3,023 squirrel sightings recorded in Central Park, New York in October 2018. It includes information such as location coordinates, age, primary and secondary fur color, elevation, activities, communications, and interactions between squirrels and with humans.\n",
    "\n",
    "There's a full description of all the columns in this dataset on the [NYC Open Data Portal](https://data.cityofnewyork.us/Environment/2018-Central-Park-Squirrel-Census-Squirrel-Data/vfnx-vebw/about_data) -- have a quick look through the table before continuing and keep it open in a separate tab to refer back to when needed!\n",
    "\n",
    "## Getting set up\n",
    "\n",
    "To get started, we need to import some key Python packages:\n",
    "\n",
    "- The ``pandas`` package is used for working with dataframes (i.e. tabular or structured data)\n",
    "- The ``numpy`` package is used for working with numerical data (you may or may not need this depending on how complex you want to get!)\n",
    "\n",
    "These packages are all widely used for data analysis. They define new data types that complement the built-in types (like dictionaries and lists), and contain a bunch of useful functions for manipulating and visualising data. The convention is to import each of these packages with a short 'nickname' that makes it quicker and easier to refer to them, as you'll see in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bafb02-56e2-4783-80ef-395289a29d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d3be7-dc4c-4304-888d-f474d22b1bc6",
   "metadata": {},
   "source": [
    "You can get an idea of how much useful stuff is inside these packages by running the following code, which will print out a list of all the different things a named package can do. Many of the entries won't make any sense at this stage -- don't worry! It's just to give you a general idea that there's *a lot* of new data types and functions now at your disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39223529-6281-4572-bbc6-a2a2bb001f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_inside(package):\n",
    "    for option in dir(package):\n",
    "        print(option)\n",
    "\n",
    "look_inside(pd) # You could also call this function with np or plt as the \"package\" argument to see the insides of those packages!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7d7dc-252c-4da7-a977-a27c61b57a04",
   "metadata": {},
   "source": [
    "Next we need to import our dataset. If you cloned the git repo for this course and haven't moved anything around, the csv file should just be in the same folder as this notebook. If you get an error in running any of the next few cells, it's probably because that's not the case -- either you just downloaded the notebook and not the dataset, or you've moved things into subfolders.\n",
    "\n",
    "As you will find with most programming languages, there are a multitude of ways in which you can load in data. Loading data efficiently is a critical step in any data analysis process, and while Python's built-in functions can handle basic data loading tasks, the pandas library offers a more powerful and flexible approach. Run the next cell to load the squirrels dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab7bb8-3917-4c0c-9af3-b001091f00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv(\"squirrels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16159da7-dc55-4269-83c1-6815a4cb2330",
   "metadata": {},
   "source": [
    "Note that pandas also has functions to import different types of datasets, like Excel files (which end with ``.xlsx``), JSON files, and SQL databases.\n",
    "\n",
    "There's also many other options within the ``read_csv`` function that we're not using here. For example:\n",
    "\n",
    "- If you have data with no header row (i.e. no column names, the data itself starts on the first row), you can specify ``header=None`` and then give a list of column names like this: ``names=[\"col1\", \"col2\", \"col3\"]`` (you can also specify ``header=0`` and then give a list of names to overwrite the existing column names).\n",
    "- If missing values in your dataset are represented in some way other than \"NA\", \"NaN\" or just an empty cell, you can use the ``na_values`` parameter to tell Python what other values to treat as NA.\n",
    "- If you're working with a particularly messy dataset, there are other options you can use to deal with errors in loading data, like ``on_bad_lines=\"skip\"``, or specifying an ``encoding`` to help Python read unusual characters.\n",
    "\n",
    "We won't go through any of these advanced options on this course, but you can always have a look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) if there's something specific you want to achieve.\n",
    "\n",
    "## Getting started with pandas dataframes\n",
    "\n",
    "Now that we've read our dataset in, we need to understand how it's actually stored in Python. When we use pandas to load data, that data gets stored as a pandas DataFrame, which is a new data type. DataFrames are two-dimensional tabular data structures i.e. they have rows and columns. First, let's create a small DataFrame from scratch to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3fee2-87eb-4913-b6d7-5254db914be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe\n",
    "cdcs = pd.DataFrame({\n",
    "    \"name\": [\"Lucia\", \"Aislinn\", \"Sarah\"],\n",
    "    \"role\": [\"Training Manager\", \"Training Fellow\", \"Training Fellow\"],\n",
    "    \"PhD\": [\"Archaeology\", \"Linguistics\", \"Data visualisation\"],\n",
    "    \"year_submitted\": [2018, 2025, 2024],\n",
    "})\n",
    "\n",
    "cdcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e68901-34c8-4611-a175-2ae135c18cbc",
   "metadata": {},
   "source": [
    "You may notice that what we pass to the ``pd.DataFrame`` function looks just like a Python dictionary, with strings as keys and lists of items as values. And what it spits out is a nice tidy table with named columns and rows. The column names are the dictionary keys, and the cells are filled in with the dictionary values in the order we specified them.\n",
    "\n",
    "To create a DataFrame like this, all the lists of items to go in the different columns must be the same length. Let's see what happens when we don't do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b299ede-cb9a-44be-9aaa-46ddade48029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 items each for the name, role and PhD columns, but only 2 values for the year_submitted column\n",
    "pd.DataFrame({\n",
    "    \"name\": [\"Lucia\", \"Aislinn\", \"Sarah\"],\n",
    "    \"role\": [\"Training Manager\", \"Training Fellow\", \"Training Fellow\"],\n",
    "    \"PhD\": [\"Archaeology\", \"Linguistics\", \"Data visualisation\"],\n",
    "    \"year_submitted\": [2018, 2025],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b6ba7-c7db-42c1-8daf-6bd1fb4ee5f1",
   "metadata": {},
   "source": [
    "Ok, that's a big error message, but the important bit is right at the bottom!\n",
    "\n",
    "In practice, it's fairly unusual to create DataFrames like this -- usually we'll be reading in an existing dataset like the squirrels one we're using on this course. So let's move on to that now.\n",
    "\n",
    "We can see what our real dataset looks like by running the following cell, which will show the first few rows (by default, 5 rows, but you can put a number in the brackets if you want to see more/fewer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92c66a-c677-4e2e-9f78-a7fd28503c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eee240-8d70-4250-b48f-6ff91c010961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the last few rows!\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa271af0-8cb0-45db-ba15-06ff8d245994",
   "metadata": {},
   "source": [
    "We can also find out the total size of our dataset using ``size`` and ``shape``. See if you can figure out the difference between these by running the next two cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf00051-1878-4358-b23c-9afe18403008",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7428fdf-a482-4dd7-bdf0-6123984c4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d5670-4852-49a3-ba97-7f445b725502",
   "metadata": {},
   "source": [
    "There a variety of other methods we can use to explore the structure of a DataFrame. For example, ``data.columns`` will show us the column names, ``data.index`` will show us the row labels, and ``data.dtypes`` will show us what data type is stored in each column. \n",
    "\n",
    "For a nice overview, we can use the ``info`` method. This provides a concise summary of the DataFrame, including the number of non-null entries and data types of each column. The ``object`` dtype refers both to strings and to columns with mixed data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e31b9-1a19-4ec0-ab31-0a7b07a94e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5ebbc-02bf-42f0-b3f3-15a70d1e4e15",
   "metadata": {},
   "source": [
    "If you want to access a specific column, you can index the DataFrame using the column name, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893a213-6271-49e2-9116-69b366bd7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Primary Fur Color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b3f4a-2a64-4dc8-bb77-5479bc5388e9",
   "metadata": {},
   "source": [
    "This returns a Series, which is another data type that's similar to a list but with \"axis labels\" (i.e. row numbers). You can also get all the values from one column as a normal list by wrapping the code above in ``list()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf175730-e708-4a0e-aec4-80b0c37a8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data[\"Primary Fur Color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aeb290-5ead-4c45-aa91-049d89e6e63f",
   "metadata": {},
   "source": [
    "To select multiple columns, you can pass a list of column names. This will return a DataFrame with the selected columns. Note that the columns don't have to be in the same order as they were in the original DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db4a6c-d076-4847-a597-cd41bf209532",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Unique Squirrel ID\", \"Primary Fur Color\", \"Date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ace57-cd6b-4a40-8cb5-f5cfdcdb8c97",
   "metadata": {},
   "source": [
    "To select a particular row by row number, you can use ``iloc``. If you have a dataset with named rows instead of numbered rows, you can use ``loc`` to select rows by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ee49e-343e-4111-b8bf-32b8a25da84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60ec74-cc65-4878-94ea-44ea942d97cf",
   "metadata": {},
   "source": [
    "To select a continguous range of rows, you can use list slicing syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb8790-dddc-41cb-bcb6-324f488dc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12494f4-6aa8-468d-860a-e14f190af4c5",
   "metadata": {},
   "source": [
    "Probably more helpfully than selecting rows by index, you can also filter based on certain criteria. For example, if you just want to see the squirrels that were recorded as being adults, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74111ff5-6d01-4ede-8388-30379f793f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "adult_squirrels = data[data[\"Age\"] == \"Adult\"]\n",
    "\n",
    "# See how many rows that leaves us with\n",
    "adult_squirrels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653061b4-b665-455c-9aa8-5cf0e9b8699c",
   "metadata": {},
   "source": [
    "You can also use more complex criteria, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee20686-a30f-4d1a-b6d2-37751e0b40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_adult_squirrels = data[(data[\"Age\"] == \"Adult\") & (data[\"Shift\"] == \"AM\")]\n",
    "morning_adult_squirrels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610594e9-ee4a-40e5-9abf-b359b88847cb",
   "metadata": {},
   "source": [
    "Have a play with these functions -- try creating some new DataFrames that contain a particular subset of the squirrels data, or creating a new DataFrame from scratch and then seeing what properties it has based on the type of data you've entered in each column. N.B. pandas DataFrames can contain all kinds of data -- in the toy example earlier we just saw strings and integers, but trying adding some floats, booleans (True or False) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9eed30-f0f7-47a3-8725-ffbdef40402b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d91b57-abd1-456d-b1fd-a5fb01a85404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e420d3b-7b52-4002-87a7-d20b1c1b389b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f72d3b-0f91-426c-b440-96b53d885b5b",
   "metadata": {},
   "source": [
    "## Data wrangling\n",
    "\n",
    "When we start working with a dataset like our squirrels one, we often have to do a lot of work to get it from its raw format into a format that's more useable for the analysis we want to do: this process is known as \"data wrangling\" (also sometimes \"data munging\", but I'll leave you to decide which term sounds nicer!). \n",
    "\n",
    "The exact steps you'll need to go through will depend on the properties of your dataset, but here's some common ones:\n",
    "\n",
    "- Cleaning text columns by removing leading/trailing whitespace, converting all characters to lowercase, removing punctuation etc.\n",
    "- Removing duplicate rows/columns.\n",
    "- Dealing with missing data and NA values (more on this later!).\n",
    "- Concatenating values from different columns to make a composite measure (the \"Combination of Primary and Highlight Color\" column in the squirrels dataset is an example of this).\n",
    "- Splitting a column out into its component parts if it's already composite.\n",
    "- Discretising continuous values into categories. For example, if we had a dataset of humans with a column for Age (in years), we might want to make a new column that labelled each person as \"child\" if they were under 18 or \"adult\" if they were over 18.\n",
    "- Moving columns around (never strictly necessary for analysis purposes, but can help make things more human readable to have related columns next to each other).\n",
    "\n",
    "The squirrels dataset is already quite well cleaned, so some of these operations won't really do anything, but we can still see how they work so you can apply them to other datasets you're working with in the future!\n",
    "\n",
    "### Text cleaning\n",
    "\n",
    "Let's have a look at text cleaning first. Whitespace at the beginning or end of strings can cause issues in data analysis (because Python will treat \"string\", \" string\" and \"string \" as three different things) -- to remove it, we can use the ``str.strip()`` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad169e1-1d8e-4e0c-823b-a8edff25519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespace from the Primary Fur Color column\n",
    "data[\"Primary Fur Color\"] = data[\"Primary Fur Color\"].str.strip()\n",
    "\n",
    "# It's standard practice to always look at the first few rows of a dataframe after applying any transformation to make sure it's worked!\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025245c-3c36-4fa3-877f-8f0880152875",
   "metadata": {},
   "source": [
    "Now this DataFrame has 31 columns, which is quite a few, but many datasets you'll be working with might have lots more than that! It's going to get pretty inefficient to do this on a column-by-column basis. There is a way to apply the same transformation to all columns containing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc71aa1-e976-4c29-a839-24573363a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.select_dtypes(include=[\"object\"]).columns] = data.select_dtypes(include=[\"object\"]).apply(lambda x: x.str.strip())\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff471e-747e-4d39-aeba-ebeeb69782cb",
   "metadata": {},
   "source": [
    "This code looks pretty horrible! But let's step through it, starting on the right of the equals sign:\n",
    "\n",
    "- ``data.select_dtypes(include=['object'])`` selects all text columns in the DataFrame (columns with type object).\n",
    "- ``.apply(lambda x: x.str.strip())`` applies ``strip()`` to each of those columns. ``lambda`` is what's called an \"anonymous function\" -- don't worry too much about this, but do ask if you want to know more!\n",
    "- The left hand side of the equals sign tells Python to store the result of that operation in each of the columns that are of type object (if you didn't include this, Python would carry out the operation and show you the result, but it wouldn't actually be saved in the DataFrame).\n",
    "\n",
    "It's also common to convert all text to lowercase (because, again, \"String\" and \"string\" are treated differently, so we want to make sure things are standardised before we start our analysis). To do this, we can use the ``str.lower()`` function in the same way as ``str.strip()``. Try doing this with just one named column or applying it to all the text columns at once. N.B. You can also chain operations (i.e. ``str.strip().str.lower()``)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69441f17-e09c-4e50-b8dc-19ecf79ce341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050ac4a-0b19-4a11-adb5-c421be914a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "274edfc5-670f-4bff-827b-0a96b66626a5",
   "metadata": {},
   "source": [
    "### Dealing with duplicate data\n",
    "\n",
    "Duplicate data can cause inaccuracies in analysis and results. It is important to identify and handle duplicates to ensure data integrity. Duplicates can occur in both rows and columns, and handling them involves either removing or aggregating the duplicate entries.\n",
    "\n",
    "Identifying duplicate rows is the first step in handling duplicate data. We can use the ``duplicated()`` method to find duplicate rows in the dataset. The `duplicated()` method returns a boolean Series indicating whether each row is a duplicate. We sum this Series to get the total number of duplicate rows and display the actual duplicate rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511697d4-1110-4beb-8c11-ff6e3120a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = data.duplicated()\n",
    "\n",
    "# Display the count of duplicate rows\n",
    "print(f'Total duplicate rows: {duplicate_rows.sum()}')\n",
    "\n",
    "# Show the duplicate rows\n",
    "print(data[duplicate_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed4825-13ca-499c-974d-33f4d327724d",
   "metadata": {},
   "source": [
    "Ok, like I said, it's a pretty clean dataset, so we don't have any duplicates! But running the following code won't do any harm, and let's you see how you'd use it in a case where you did have duplicates.\n",
    "\n",
    "Once duplicates are identified, we can remove them using the `drop_duplicates()` method. This method removes duplicate rows, keeping the first occurrence by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfd0df-34ad-431b-b267-f9b4558aa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0e32f-d5dc-40da-ace5-299f48a6f016",
   "metadata": {},
   "source": [
    "Duplicate columns can also occur, especially when merging datasets. We can identify duplicate columns by comparing the values across columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f8384-b0bf-4887-948f-863baa421151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate columns by transposing the dataframe and using duplicated\n",
    "duplicate_columns = data.T.duplicated()\n",
    "\n",
    "# Display the duplicate columns\n",
    "print(f'Duplicate columns: {duplicate_columns.sum()}')\n",
    "print(data.T[duplicate_columns].T.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7824266-7ceb-4875-8b5f-46a9c43d0758",
   "metadata": {},
   "source": [
    "We transpose the DataFrame and use the ``duplicated()`` method to identify duplicate columns. Transposing the DataFrame switches rows and columns, allowing us to use the same method for detecting duplicates.\n",
    "\n",
    "After identifying duplicate columns, we can remove them by selecting only the unique columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffe82f-6161-4aaf-b6f4-361d7aba3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ~data.T.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c18d5-23ec-4c23-b416-d56d6609501c",
   "metadata": {},
   "source": [
    "We remove duplicate columns by transposing the DataFrame, removing the duplicate columns, and transposing it back. This ensures that we retain only unique columns in the dataset.\n",
    "\n",
    "Sometimes, rows may not be exact duplicates but may still need to be handled due to partial duplication. We can use the `drop_duplicates()` method with specific columns to handle this, by passing a list of columns to the `subset` parameter to specify which columns to consider when identifying duplicates. This allows us to handle partial duplicates based on specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111f489-071b-4042-b95d-a4a261ae54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data = data.drop_duplicates(subset=[\"Unique Squirrel ID\"]) # Maybe we only want to keep one row per squirrel\n",
    "unique_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04eb8f2-52c3-4e7d-b05f-a6269b101d23",
   "metadata": {},
   "source": [
    "### Transforming data into different types\n",
    "\n",
    "Another thing I noticed about the squirrels dataset is that the Date column is just being treated as a number, not a real date -- if you look at the [NYC Open Data Portal](https://data.cityofnewyork.us/Environment/2018-Central-Park-Squirrel-Census-Squirrel-Data/vfnx-vebw/about_data) you'll see that this column is a \"concatenation of the sighting session day and month\". Python has a package, ``datetime``, which is specifically designed to work with dates, so we might want to transform this column into that data type. Let's give it a go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05c99c-b4a2-477e-b4d5-afab8b553086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import the datetime package\n",
    "import datetime as dt\n",
    "\n",
    "# Then we can add a new column where we convert the date column into a datetime object\n",
    "# The format argument tells Python how the elements are ordered in the original column (this is an American dataset so it's MM/DD/YYYY!)\n",
    "data[\"parsed_date\"] = pd.to_datetime(data[\"Date\"], format=\"%m%d%Y\")\n",
    "\n",
    "# And then if we have a look at info() again we'll see the new column at the bottom with the datetime dytpe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277ff7d-feb6-45a2-be94-117806bd7662",
   "metadata": {},
   "source": [
    "### Other tasks you can think of!\n",
    "\n",
    "Have a think about some other changes or additions you'd like to make to this dataset (or any others you've brought to work on). Discuss with your partner/others at your table how you think you might go about implementing these. If you feel confident navigating the pandas documentation yourself, have a go at running your own code in the next few cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976105fd-8bd9-4eda-a539-a911a7e19c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aecb81-7d3f-4ee8-b51e-1dbc2bce2130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ba2bb-81ee-4465-ab22-8b95ed354f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555b1c55-af7d-48df-88b5-f509db5bdc9c",
   "metadata": {},
   "source": [
    "## Dealing with missing data\n",
    "\n",
    "You may have noticed when we ran ``data.info()`` that there's quite a lot of missing data in some columns; for example, that output told us that there were only 182 entries in the \"color notes\" column out of a total of 3023 rows.\n",
    "\n",
    "Another way to look at this is to count how many missing values are in each column by chaining two different operations. ``isnull()`` returns a DataFrame of the same shape as the original, but with boolean values indicating the presence of missing data (try removing ``.sum()`` to see what this looks like!). Chaining the ``sum()`` method allows us to count the number of missing values in each column, which gives a clear overview of where data is missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cf82c-8d3b-444f-b815-d09854c9791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5742e75-0501-4334-a877-c9a1398f9188",
   "metadata": {},
   "source": [
    "There's no one way to deal with missing data. Sometimes, the best thing to do will be to just remove certain rows or columns, but other times, you'll want to try and fill in missing values. Each approach has trade-offs, and the best method depends on the specific dataset and analysis requirements.\n",
    "\n",
    "We can use the ``dropna()`` function to remove all rows or columns with missing values at once, but based on the summary above, this might not leave us with much data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144dabc3-6414-4b6f-9fb8-bec2256cbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with any missing values\n",
    "data_dropped_rows = data.dropna()\n",
    "\n",
    "# Remove all columns with any missing values\n",
    "data_dropped_columns = data.dropna(axis=1) # axis=1 tells dropna() to go by columns instead of rows (its default behaviour)\n",
    "\n",
    "# Check how many rows this leaves us with\n",
    "print(\"Original dataset shape:\", data.shape)\n",
    "print(\"After dropping rows:\", data_dropped_rows.shape)\n",
    "print(\"After dropping columns:\", data_dropped_columns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1756c-e7b6-4851-9501-7cb617359c42",
   "metadata": {},
   "source": [
    "Just dropping rows/columns where there's **any** missing data seems perhaps a bit extreme in this case. Instead, I think I'll just remove a few of the columns that have a lot of missing data using ``drop()``, and fill in some others with a default value using ``fillna()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fd6f5-2f2d-42c8-8f05-41f92bb65e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with a lot of missing data\n",
    "# N.B. inplace=True means the result of this operation will be saved in the dataframe without us explicitly assigning it using data = data.drop()\n",
    "data.drop(columns=[\"Color notes\", \"Specific Location\", \"Other Activities\", \"Other Interactions\"], inplace=True)\n",
    "\n",
    "# Check how many columns we've got now (should be less than before!)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e46f7-9708-479c-93cf-cf931ffd7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing cells in text columns with \"unknown\"\n",
    "data[data.select_dtypes(include=[\"object\"]).columns] = data.select_dtypes(include=[\"object\"]).fillna(\"unknown\")\n",
    "\n",
    "# Visually inspect the data (we should see \"unknown\" in some cells now since we had NA values on the first few rows before)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5f928-3424-410c-8ec0-e5516e2c0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And check whether we still have any null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085ce68-291a-4b66-bc31-a5d7a4fae3a4",
   "metadata": {},
   "source": [
    "We'll see another possible method of dealing with missing data next week when we get on to summary statistics: filling empty cells with a computed statistic (e.g. the mean of the column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4a16b-4b1a-490e-a5ca-281b46d9bd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864d1d3-317e-4a6b-a986-4d39ee4d2157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf1dd9-806c-4b47-bb2e-91f517a90bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
